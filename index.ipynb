{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Machine Learning with Scikit-Learn:<br>Introduction to Machine Learning\n",
    "## Jake VanderPlas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# The code in these cells is runable. \n",
    "# Click on this cell, then press Shift+Enter to run it, \n",
    "# or click the Run button in the toolbar.\n",
    "\n",
    "print(\"Hello, World!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook will cover the basics of scikit-learn, a popular package containing a collection of tools for machine learning written in Python. See more at <a href=\"http://scikit-learn.org\" target=\"_blank\"><em>http://scikit-learn.org</em></a>."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Outline\n",
    "\n",
    "This first notebook in the series covers the basics of two key concepts:\n",
    "\n",
    "- What is machine learning?\n",
    "- How do you do basic machine learning in scikit-learn?\n",
    "\n",
    "These provide the background required to understand the deeper dives into machine learning methods that we will see in later parts of this series.\n",
    "\n",
    "By the end of this notebook, the viewer will:\n",
    "\n",
    "- Have an intuitive understanding of the basic categories and tasks of machine learning.\n",
    "- Understand the data layout expected in scikit-learn.\n",
    "- Understand essential features of scikit-learnâ€™s API and how they are used in the case of simple datasets.\n",
    "- See one example of how these API features can be used together to gain insight from a more complicated dataset.\n",
    "\n",
    "**Main Goal:** To introduce the central concepts of machine learning, and how they can be applied in Python using the scikit-learn package.\n",
    "\n",
    "- Definition of machine learning\n",
    "- Data representation in scikit-learn\n",
    "- Introduction to the scikit-learn API"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# What Is Machine Learning?\n",
    "\n",
    "In this section, we will begin to explore the basic principles of machine learning.\n",
    "Machine learning is about building programs with *tunable parameters* (typically an\n",
    "array of floating point values) that are adjusted automatically so as to improve\n",
    "their behavior by *adapting to previously seen data.*\n",
    "\n",
    "Machine learning can be considered a subfield of *artificial intelligence*, since those\n",
    "algorithms can be seen as building blocks to make computers learn to behave more\n",
    "intelligently by *generalizing based on data* rather than just storing and retrieving data items\n",
    "like a database system might do."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Simple Machine Learning Example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# This call is required in notebooks to make figures appear within the notebook\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# lesson1.py is some code designed to keep this part of the discussion\n",
    "# qualitative, rather than technical. Feel free to dig in if you're\n",
    "# curious how these things are implemented! Uncomment the last line\n",
    "# to see the source code.\n",
    "import lesson1\n",
    "# lesson1??"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As a concrete example, consider this data: we have data with two numerical *features*, represented by the values of the x and y axes:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from lesson1 import labeled_data, unknown_data\n",
    "labeled_data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we visualize the data, it looks like this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "lesson1.plot_data(labeled_data);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you were trying to create a model to let you distinguish between red and blue points, you might start by drawing a line between groups of points, maybe something like this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from lesson1 import interactive_model\n",
    "interactive_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "interactive_model.kwargs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Given this trained model, when you encounter some new, unknown data, you can use this line to label the new data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "unknown_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "lesson1.plot_data(unknown_data);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "lesson1.plot_fit_model(unknown_data, **interactive_model.kwargs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This example, in which we are predicting *discrete* labels from numerical features, is known as a *Classification* task.\n",
    "\n",
    "Note the steps we went through (some implicitly):\n",
    "\n",
    "1. Find some labeled *training data*.\n",
    "2. Choose a model type (here, we implicitly chose \"a straight line divides the classes\").\n",
    "3. Tune the model parameters until it fits the training data (i.e., adjust the slope and angle of the line).\n",
    "4. Given this tuned model, make predictions on unknown data.\n",
    "\n",
    "This may seem like a trivial task, but it is a simple version of a very important concept.\n",
    "By drawing this separating line, we have learned a model that can *generalize* to new\n",
    "data: if you were to drop another point onto the plane that is unlabeled, this algorithm\n",
    "could now *predict* whether it's a blue or a red point.\n",
    "\n",
    "In machine learning, the \"training\" step that we did by hand (i.e., adjusting the model until it fits the training data) is done automatically based on one of a variety of algorithms.\n",
    "\n",
    "For example, here's how the above procedure can be done automatically in scikit-learn:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.svm import LinearSVC\n",
    "\n",
    "# 1. Format the training data\n",
    "features = labeled_data[['feature1', 'feature2']]\n",
    "labels = labeled_data['color']\n",
    "\n",
    "# 2. Choose the model: here a linear support vector classifier\n",
    "model = LinearSVC()\n",
    "\n",
    "# 3. Fit the model parameters to labeled training data\n",
    "model.fit(features, labels)\n",
    "\n",
    "# 4. Predict the colors for the unknown data\n",
    "features = unknown_data[['feature1', 'feature2']]\n",
    "unknown_data['color'] = model.predict(features)\n",
    "\n",
    "# Now we can visualize the results\n",
    "lesson1.plot_data(unknown_data);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "That is machine learning in a nutshell! In the remainder of this notebook, and the ones that follow it in the series, we will be diving into each of these steps in more detail. We will also talk about how to evaluate the effectiveness of your models."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Formatting and Representing Data\n",
    "\n",
    "Machine learning is about creating models from data. For that reason, we'll start by\n",
    "discussing how data can be represented in order to be understood by the computer.  Along\n",
    "with this, we'll build on our matplotlib examples from the previous section and show some\n",
    "examples of how to visualize data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Most machine learning algorithms implemented in Scikit-learn expect data to be stored in a\n",
    "*two-dimensional array or matrix*.  The arrays can be\n",
    "either ``numpy`` arrays, or in some cases ``scipy.sparse`` matrices.\n",
    "The size of the array is expected to be `[n_samples, n_features]`\n",
    "\n",
    "- `n_samples`:  The number of samples: each sample is an item to process (e.g., classify).\n",
    "  A sample can be a document, a picture, a sound, a video, an astronomical object,\n",
    "  a row in database or CSV file,\n",
    "  or whatever you can describe with a fixed set of quantitative traits.\n",
    "- `n_features`:  The number of features or distinct traits that can be used to describe each\n",
    "  item in a quantitative manner.  Features are generally real-valued, but may be boolean or\n",
    "  discrete-valued in some cases.\n",
    "\n",
    "The number of features must be fixed in advance. However, it can be very high dimensional\n",
    "(e.g., millions of features), with most of them being zeros for a given sample. This is a case\n",
    "where `scipy.sparse` matrices can be useful, in that they are\n",
    "much more memory-efficient than NumPy arrays."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "lesson1.data_layout_figure()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(Figure from the <a href=\"https://jakevdp.github.io/PythonDataScienceHandbook\" target=\"_blank\"><em>Python Data Science Handbook</em></a>)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As an example of a simple dataset, let's consider the task of classifying flowers by species.\n",
    "A trained botanist can go out into the wild, find a flower, and determine the species of the flower by looking at various characteristics.\n",
    "\n",
    "Can we train a machine learning model to do the same?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src='images/iris.jpg'>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The first step in machine learning is to map the real world objects onto an ``[n_samples, n_features]`` matrix of numerical features.\n",
    "Some approaches you might use here include:\n",
    "\n",
    "- Recording the color of each flower and particular points (e.g., reflectivity in red, green, or blue)\n",
    "- Recording the size of particular flower features (e.g., length and width of petals, height of stalk, etc.)\n",
    "- Using the photograph itself as data (e.g., pixel values in a 100x100 image of each flower)\n",
    "\n",
    "Choosing the \"best\" features for a particular dataset can often be helped by domain expertise (in this case, consulting with a botanist).\n",
    "\n",
    "Additionally, we will need some \"ground truth\" labels to work with. For example, we could ask an expert to classify several hundred flowers, and then train the algorithm with that training set."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The standard Iris dataset makes some straightforward choices for this. It uses the following:\n",
    "\n",
    "**Features in the Iris dataset:**\n",
    "\n",
    "  1. Sepal length in cm\n",
    "  2. Sepal width in cm\n",
    "  3. Petal length in cm\n",
    "  4. Petal width in cm\n",
    "\n",
    "**Target classes to predict:**\n",
    "\n",
    "  1. Iris Setosa\n",
    "  2. Iris Versicolour\n",
    "  3. Iris Virginica\n",
    "  \n",
    "This data can be found in several places; I find the <a href=\"http://seaborn.pydata.org/\" target=\"_blank\">seaborn data fetcher</a> to be convenient:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import seaborn\n",
    "iris = seaborn.load_dataset('iris')\n",
    "iris.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When confronted with any dataset, it is often useful to start by visualizing it; here we'll use seaborn's <a href=\"http://seaborn.pydata.org/generated/seaborn.pairplot.html\" target=\"_blank\"><code>pairplot</code></a> functionality:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "seaborn.pairplot(iris, hue='species');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we must extract the features matrix and target vector from the data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "features = iris.drop('species', axis=1)\n",
    "target = iris['species']\n",
    "\n",
    "print(features.shape)\n",
    "print(target.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Choosing a Model\n",
    "\n",
    "With your dataset in order, the next step is to choose a model for your data.\n",
    "Part of the art of machine learning is developing the intuition for which model is appropriate where, but the process is decently summarized by this flowchart from the scikit-learn documentation:\n",
    "\n",
    "<img src='images/sklearn_flowchart.png'>\n",
    "\n",
    "(Interactive version available <a href=\"http://scikit-learn.org/stable/tutorial/machine_learning_map/index.html\" target=\"_blank\">here</a>.)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we follow the flowchart from \"START\" for this data, we land on the Linear SVC estimator (i.e., we have more than 50 samples, are predicting a category, have labeled data, and fewer than 100K samples).\n",
    "To prepare this model, we import the model object from scikit-learn, and we instantiate the class:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.svm import LinearSVC\n",
    "model = LinearSVC()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "All scikit-learn estimators are implemented as Python classes, and work in this way.\n",
    "\n",
    "An important point is that you instantiate the class without reference to any data. This is equivalent to above when our model was \"a line will fit the data\", but we have not yet chosen the slope and intercept.\n",
    "\n",
    "This model is poised and ready to be fed data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Fitting the Model\n",
    "\n",
    "To fit the model to training data, we call the ``fit()`` method. For classification tasks, the ``fit()`` method requires a feature matrix and a target vector, which we fortunately prepared earlier:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model.fit(features, target)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Inspecting the model now can be useful, and we can do this with the Jupyter notebook's tab-completion features:\n",
    "\n",
    "<img src='images/tab-completion.png' width=400>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# type \"model.\" and then press the Tab key to see the list of options\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For example, you find the value of model hyperparameters like ``fit_intercept``:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model.fit_intercept"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In particular, any attribute that ends with an underscore, by convention, is fit from the data.\n",
    "For example, here ``model.coef_`` is analogous to the slope and intercept in our simple model above:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model.coef_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "At this point, you can treat the details as a black box; we'll discuss the idea behind SVM models in a later lesson (_Machine Learning with Scikit-Learn: Support Vector Machines_)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Predicting on New Data\n",
    "\n",
    "Once a model is fit, you can use the ``predict()`` method to apply the model to new data.\n",
    "For example, let's see how well the model recovers the labels of the training data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "predicted = model.predict(features)\n",
    "predicted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "(predicted == target).mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here our model is about 99% effective at recovering labels *for objects it has already seen*.\n",
    "As we will see later, this is not a good indicator of how well it might perform for objects it has not yet seen, but we will come to that discussion in a later lesson (_Machine Learning with Scikit-Learn: Hyperparameters and Model Validation_).\n",
    "\n",
    "This *accuracy* measure is important enough that scikit-learn provides a convenience routine to compute it:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "accuracy_score(predicted, target)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Estimator Interface: Summary\n",
    "\n",
    "This is essentially what you need to know about scikit-learn estimators:\n",
    "\n",
    "- Choosing the model is accomplished by instantiating an estimator class\n",
    "- Fitting the data is done with the ``fit()`` method\n",
    "- Predicting labels for new data is done with the ``predict()`` method\n",
    "\n",
    "A benefit of scikit-learn's interface is that new models can be dropped in without changing code! For example, we can try several different models with the same code:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model = LinearSVC()\n",
    "model.fit(features, target)\n",
    "predicted = model.predict(features)\n",
    "print(model.__class__.__name__, (predicted == target).mean())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By simply changing the model class you use, you can reuse the same code with vastly different models.\n",
    "For example, you might try each of the following models:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "\n",
    "model = LinearSVC()  # Change this!\n",
    "\n",
    "model.fit(features, target)\n",
    "predicted = model.predict(features)\n",
    "print(model.__class__.__name__, (predicted == target).mean())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can even write a loop to do this all at once:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "for Model in [LinearSVC, RandomForestClassifier, LogisticRegression,\n",
    "              GaussianNB, KNeighborsClassifier]:\n",
    "    model = Model()\n",
    "    model.fit(features, target)\n",
    "    predicted = model.predict(features)\n",
    "    print(model.__class__.__name__, (predicted == target).mean())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Again, we are predicting labels for data the models have already seen, so these numbers are not particularly useful in evaluating model effectiveness, but we will return to that later."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Other Categories of Machine Learning\n",
    "\n",
    "So far we have focused on a particular type of task: predicting discrete labels. This is known as *Classification*, which is a subcategory of **supervised learning**. Here are other categorizations of machine learning you should become familiar with:\n",
    "\n",
    "- **Supervised learning**: Using *labeled* training data to predict new labels on unlabeled data\n",
    "\n",
    "  - *Classification*: Labels are discrete categories\n",
    "  - *Regression*: Labels are continuous quantities\n",
    "  \n",
    "  \n",
    "- **Unsupervised learning**: Using *unlabeled* training data to model relationship within the data\n",
    "\n",
    "   - *Clustering*: Identifying groups of similar data\n",
    "   - *Dimensionality Reduction*: Identifying a small number of features or combinations of features that are sufficient to describe the dataset\n",
    "   - *Density Estimation*: Predicting the probability that an object is consistent with the training data\n",
    "   \n",
    "In all these cases, the scikit-learn estimator syntax is similar."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Quick Application: Optical Character Recognition"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To demonstrate the above principles on a more interesting problem, let's consider OCR (Optical Character Recognition)â€”that is, recognizing hand-written digits.\n",
    "In the wild, this problem involves both locating and identifying characters in an image. Here we'll take a shortcut and use scikit-learn's set of preformatted digits, which is built into the library."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading and Visualizing the Digits Data\n",
    "\n",
    "We'll use scikit-learn's data access interface to take a look at this data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn import datasets\n",
    "digits = datasets.load_digits()\n",
    "digits.images.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's plot a few of these:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(10, 10, figsize=(8, 8))\n",
    "fig.subplots_adjust(hspace=0.1, wspace=0.1)\n",
    "\n",
    "for i, ax in enumerate(axes.flat):\n",
    "    ax.imshow(digits.images[i], cmap='binary', interpolation='nearest')\n",
    "    ax.text(0.05, 0.05, str(digits.target[i]),\n",
    "            transform=ax.transAxes, color='green')\n",
    "    ax.set_xticks([])\n",
    "    ax.set_yticks([])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here, the data is simply each pixel value within an 8x8 grid:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# The images themselves\n",
    "print(digits.images.shape)\n",
    "print(digits.images[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "plt.imshow(digits.images[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# The data for use in our algorithms\n",
    "print(digits.data.shape)\n",
    "print(digits.data[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# The target label\n",
    "print(digits.target)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So our dataset has 1797 samples in 64 dimensions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Unsupervised Learning: Dimensionality Reduction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When confronted with a new dataset, it is useful to visualize it in order to gain intuition about the nature of the data.\n",
    "We could attempt to visualize our points within the 64-dimensional parameter space, but it's difficult to plot points in 64 dimensions!\n",
    "\n",
    "Instead we'll reduce the dimensions using an unsupervised method; we'll make use of a manifold learning algorithm called *Isomap* to transform the data to two dimensions.\n",
    "\n",
    "Again, the goal here is to identify a small number of features or combinations of features that are sufficient to describe the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.manifold import Isomap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "iso = Isomap(n_components=2)\n",
    "iso.fit(digits.data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In place of the ``predict()`` method of supervised estimators, dimensionality reduction estimators have a ``transform`` method:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data_projected = iso.transform(digits.data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The result is a dataset with two features that suitably describe the dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data_projected.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "plt.scatter(data_projected[:, 0], data_projected[:, 1], c=digits.target,\n",
    "            edgecolor='none', alpha=0.5, cmap=plt.cm.get_cmap('nipy_spectral', 10));\n",
    "plt.colorbar(label='digit label', ticks=range(10))\n",
    "plt.clim(-0.5, 9.5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see here that the digits are fairly well-separated in the parameter space; this tells us that a supervised classification algorithm should perform fairly well. Let's give it a try."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Classification on Digits\n",
    "\n",
    "Let's try a classification task on the digits. The first thing we'll want to do is split the digits into a training and testing sample; this lets us see how the classification might fare on data that it has not been trained on:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "Xtrain, Xtest, ytrain, ytest = train_test_split(digits.data, digits.target,\n",
    "                                                random_state=2)\n",
    "print(Xtrain.shape, Xtest.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's use a ``LinearSVC`` classifier, as we did above:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.svm import LinearSVC\n",
    "clf = LinearSVC()\n",
    "clf.fit(Xtrain, ytrain)\n",
    "ypred = clf.predict(Xtest)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can check our classification accuracy by comparing the true values of the test set to the predictions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "accuracy_score(ytest, ypred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This single number doesn't tell us where we've gone wrong: one nice way to discover this is to use the *confusion matrix*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "mat = confusion_matrix(ytest, ypred)\n",
    "\n",
    "seaborn.heatmap(mat, square=True, annot=True, cbar=False, cmap='Blues')\n",
    "plt.xlabel('predicted value')\n",
    "plt.ylabel('true value');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We might also take a look at some of the outputs along with their predicted labels. We'll make the bad labels red:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(10, 10, figsize=(8, 8))\n",
    "fig.subplots_adjust(hspace=0.1, wspace=0.1)\n",
    "\n",
    "for i, ax in enumerate(axes.flat):\n",
    "    ax.imshow(Xtest[i].reshape(8, 8), cmap='binary')\n",
    "    ax.text(0.05, 0.05, str(ypred[i]),\n",
    "            transform=ax.transAxes,\n",
    "            color='green' if (ytest[i] == ypred[i]) else 'red')\n",
    "    ax.set_xticks([])\n",
    "    ax.set_yticks([])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The interesting thing is that even with this simple logistic regression algorithm, many of the mislabeled cases are ones that we ourselves might get wrong!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You might also be interested in trying a few of the other classifiers mentioned above, to see if any of them do a better job with their classification."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Try it here!\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Summary\n",
    "\n",
    "In this notebook, we have covered the essential features of the scikit-learn data representation and the estimator API.\n",
    "Regardless of the type of estimator, the same import/instantiate/fit/predict pattern holds.\n",
    "Armed with this information about the estimator API, you can explore the scikit-learn documentation and start trying out various models on your data."
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
